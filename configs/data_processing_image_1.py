import os
import json
from PIL import Image
from datasets import Dataset, load_from_disk
from transformers import AutoTokenizer


# === 1. åŠ è½½ JSON æ•°æ®ï¼ˆåŒ…å« image_path å’Œæ–‡æœ¬ä¿¡æ¯ï¼‰ ===
with open('/home/Userlist/kongzicheng/omni-critic-r1/selected preference dataset/image/doubao_image_3b_7b_output.json', 'r') as f:
    json_data = json.load(f)

# === 2. åŠ è½½ Arrow å›¾åƒæ•°æ®é›† ===
arrow_dataset = load_from_disk("/home/Userlist/kongzicheng/omni-critic-r1/rlaif-v-dataset/train")

# === 3. æ„å»º image_path â†’ row_index æ˜ å°„ï¼Œé¿å…ä¸€æ¬¡æ€§è§£ç å›¾åƒ ===
image_index = {
    path: i for i, path in enumerate(arrow_dataset["image_path"])
}
print(f"âœ… Image index built: {len(image_index)} entries")

# === 4. åŠ è½½ Qwen åˆ†è¯å™¨ï¼ˆä»…ä½¿ç”¨ tokenizerï¼‰ ===
tokenizer = AutoTokenizer.from_pretrained("/home/Userlist/kongzicheng/omni-critic-r1/Qwen2.5-Omni-3B/qwen/Qwen2.5-Omni-3B")
print("âœ… Tokenizer loaded.")

# === 5. æ„å»º Datasetï¼ˆæ¥è‡ª JSONï¼‰ ===
dataset = Dataset.from_dict({
    "image_path": [entry["image_path"] for entry in json_data],
    "question": [entry["question"] for entry in json_data],
    "qwen2_5_vl_3b_output": [entry["qwen2_5_vl_3b_output"] for entry in json_data],
    "qwen2_5_vl_7b_output": [entry["qwen2_5_vl_7b_output"] for entry in json_data],
    "score_A": [entry["score_A"] for entry in json_data],
    "score_B": [entry["score_B"] for entry in json_data],
    "better": [entry["better"] for entry in json_data],
    "reasoning": [entry["reasoning"] for entry in json_data],
    "final_verdict": [entry["final_verdict"] for entry in json_data],
    "identical": [entry["identical"] for entry in json_data]
})
print("âœ… JSON dataset loaded with", len(dataset), "entries.")

# === 6. å›¾åƒåŠ è½½å‡½æ•°ï¼ˆæ ¹æ® image_path ä» Arrow æ•°æ®ä¸­æŸ¥æ‰¾ï¼‰ ===
def load_image_by_path(image_path):
    try:
        idx = image_index.get(image_path, None)
        if idx is None:
            raise ValueError("Image path not found")
        image = arrow_dataset[idx]["image"]
        return image.convert("RGB")
    except Exception as e:
        print(f"âš ï¸ Error loading image: {image_path} | {e}")
        return None

# === 7. æ•°æ®é¢„å¤„ç†å‡½æ•° ===
def preprocess_data(example):
    image = load_image_by_path(example['image_path'])

    question_encoding = tokenizer(example['question'], truncation=True, padding="max_length", max_length=256)
    answer_3b_encoding = tokenizer(example['qwen2_5_vl_3b_output'], truncation=True, padding="max_length", max_length=1024)
    answer_7b_encoding = tokenizer(example['qwen2_5_vl_7b_output'], truncation=True, padding="max_length", max_length=1024)

    return {
        'image': image,
        'question': question_encoding,
        'answer_3b': answer_3b_encoding,
        'answer_7b': answer_7b_encoding,
        'score_A': example['score_A'],
        'score_B': example['score_B'],
        'better': example['better'],
        'reasoning': example['reasoning'],
        'final_verdict': example['final_verdict'],
        'identical': example['identical']
    }

# === 8. åº”ç”¨é¢„å¤„ç† ===
print("ğŸš€ Starting dataset preprocessing...")
dataset = dataset.map(preprocess_data)
print("âœ… Preprocessing complete.")

# === 9. åˆ’åˆ†è®­ç»ƒé›† / éªŒè¯é›† ===
split = dataset.train_test_split(test_size=0.2)
train_dataset = split['train']
val_dataset = split['test']
print(f"âœ… Train set: {len(train_dataset)}, Validation set: {len(val_dataset)}")

# === 10. ä¿å­˜è‡³ç£ç›˜ ===
output_dir = '/home/Userlist/kongzicheng/omni-critic-r1/OmniCritic-SFT/sft_dataset/image/3b_7b'
train_dataset.save_to_disk(os.path.join(output_dir, "train"))
val_dataset.save_to_disk(os.path.join(output_dir, "val"))
print("ğŸ’¾ Datasets saved to:", output_dir)




